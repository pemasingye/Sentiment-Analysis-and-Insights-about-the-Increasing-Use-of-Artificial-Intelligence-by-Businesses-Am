{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install twint\n",
    "!pip install tweepy\n",
    "!pip install -U textblob\n",
    "!pip install emoji --upgrade\n",
    "!pip install --upgrade pip\n",
    "!pip install wordcloud\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##ADD THIS INSTALL\n",
    "!pip3 install --user --upgrade git+https://github.com/yunusemrecatalcam/twint.git@twitter_legacy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import pandas as pd\n",
    "import twint\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA IMPORTATION (At this stage you should have already cleaned your data) \n",
    "You can't do data clean with this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your data in the next cell# Use example pd.read_csv(\"COVID-19INNOVATION.csv\") if your file is CSV \n",
    "#and use pd.read_excel(\"COVID-19INNOVATION.xlsx\")\n",
    "#the df is the title of your data. You have to change this in accordance to whatever title you used and make sure to replace df in all the cell with your title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"COVID-19INNOVATION.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "  \n",
    "# iterate through the csv file \n",
    "for val in df.cleaned_tweets: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 1000, height = 800, \n",
    "                background_color='white', colormap='Set2', \n",
    "                collocations=False, \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 12).generate(comment_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (10, 10), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(text):\n",
    "    return TextBlob( str(text)).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(text):\n",
    "    return TextBlob( str(text)).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['cleaned_tweets'], inplace = True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subjectivity'] = df['cleaned_tweets'].apply(getSubjectivity)\n",
    "df['Polarity'] = df['cleaned_tweets'].apply(getPolarity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a function to compute negative (-1), neutral (0) and positive (+1) analysis\n",
    "def get_Polarity_Analysis(score):\n",
    "    if score < 0:\n",
    "      return 'Negative'\n",
    "    elif score == 0:\n",
    "      return 'Neutral'\n",
    "    else:\n",
    "      return 'Positive'\n",
    "def get_Subjectivity_Analysis(score):\n",
    "    if score >  0:\n",
    "      return 'Opinion'\n",
    "    else:\n",
    "      return 'Fact'\n",
    "\n",
    "df['Analysis_Polarity'] = df['Polarity'].apply(get_Polarity_Analysis)\n",
    "\n",
    "df['Analysis_Subjectivity'] = df['Subjectivity'].apply(get_Subjectivity_Analysis)\n",
    "\n",
    "# Show the dataframe\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"PolaritySubjectivityInnovation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10)) \n",
    "\n",
    "# plt.style.use('seaborn-pastel')\n",
    "\n",
    "plt.scatter(df['Polarity'], df['Subjectivity'], c=df['Polarity'], s=100, cmap='RdYlGn') \n",
    "\n",
    "plt.xlim(-1.1, 1.1)\n",
    "plt.ylim(-0.1, 1.1) \n",
    "plt.title('Sentiment Analysis') \n",
    "plt.xlabel('Polarity') \n",
    "plt.ylabel('Subjectivity') \n",
    "plt.show(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the value counts\n",
    "df['Analysis_Polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting and visualizing the counts\n",
    "plt.figure(figsize=(15,10)) \n",
    "\n",
    "plt.title('Polarity Sentiment Analysis')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Counts')\n",
    "df['Analysis_Polarity'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_polarity = [p for p in df['Polarity'] if p>0]\n",
    "negative_polarity = [n for n in df['Polarity'] if n<0]\n",
    "neutral_polarity = [r for r in df['Polarity'] if r==0]\n",
    "\n",
    "total_size = len(positive_polarity) + len(negative_polarity) + len(neutral_polarity)\n",
    "n_size = len(negative_polarity)/total_size\n",
    "p_size = len(positive_polarity)/total_size\n",
    "r_size = len(neutral_polarity)/total_size\n",
    "\n",
    "labels = ['Neutral tweets', 'Positive tweets', 'Negative tweets']\n",
    "sizes = [r_size, p_size, n_size]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "\t        shadow=False, startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting and visualizing the counts\n",
    "plt.figure(figsize=(15,10)) \n",
    "\n",
    "plt.title('Subjectivity Sentiment Analysis')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Counts')\n",
    "df['Analysis_Subjectivity'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the value counts\n",
    "df['Analysis_Subjectivity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(df['cleaned_tweets'].values.astype('U'))\n",
    "print(dtm_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "dtm_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_tweets'].values.astype('U'))\n",
    "print(dtm_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TF DTM\n",
    "lda_tf = LatentDirichletAllocation(n_components =8, random_state=50)\n",
    "lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "lda_tfidf = LatentDirichletAllocation(n_components =8, random_state=50)\n",
    "lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,topic in enumerate(lda_tf.components_):\n",
    "#print([tfidf_vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "#print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,topic in enumerate(lda_tf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vectorizer.get_feature_names()[i] for i in topic.argsort()[-30:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_values = lda_tf.transform(dtm_tf)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TF DTM\n",
    "#lda_tf = LatentDirichletAllocation(n_components =6, random_state=42)\n",
    "#lda_tf.fit(dtm_tf)\n",
    "# for TFIDF DTM\n",
    "#lda_tfidf = LatentDirichletAllocation(n_components =6, random_state=42)\n",
    "#lda_tfidf.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=df.replace({0:'Webinar',1:'Learning',2:'Rwanda',3:'Healthcare',4:'Technology',5:'Coronavirus',6:'Medical',7:'Challenge'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.figure(figsize=(40,25)) \n",
    "\n",
    "g=sns.lmplot(x=\"Polarity\", y=\"Subjectivity\", hue='Topic', data=df, fit_reg=False, legend=False,palette=\"GnBu_d\", col='Topic', legend_out=True)\n",
    " \n",
    "# # Move the legend to an empty part of the plot\n",
    "# plt.legend(loc='lower right')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2 = df_1.groupby(['Topic'])['Analysis_Polarity'].value_counts().unstack('Topic').transpose()\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Total'] = df2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df2:\n",
    "    df2[i] = round(df2[i]*100/df2.Total)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct Polarity Topic Analysis using Tableau\n",
    "\n",
    "df.to_excel(\"Polarity_Topic Modeling.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df2.drop(['Total'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting and visualizing the counts\n",
    "plt.figure(figsize=(20,15)) \n",
    "\n",
    "\n",
    "topic = ['Webinar','Learning','Rwanda','Healthcare','Technology','Coronavirus','Medical','Challenge']\n",
    "sentiment = ['Negative', 'Neutral', 'Positive']\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('%')\n",
    "p1=plt.bar(topic,df2['Negative'], color='#23646e')\n",
    "p2=plt.bar(topic,df2['Neutral'], color='#419dab', bottom=df2['Negative'])\n",
    "p3=plt.bar(topic,df2['Positive'], color='#6bbbc7', bottom=df2['Neutral']+df2['Negative'])\n",
    "plt.xticks(topic, rotation=90)\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.legend((p1[0], p2[0], p3[0]),('Negative', 'Neutral', 'Positive'),fontsize=12, ncol=4, framealpha=0, fancybox=True, loc='upper center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace({0:'Webinar',1:'Learning',2:'Rwanda',3:'Healthcare',4:'Technology',5:'Coronavirus',6:'Medical',7:'Challenge'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the value counts\n",
    "df['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15)) \n",
    "sns.countplot(x = 'Topic',data = df, palette = 'viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREND ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date range with frequency of a day\n",
    "rng = pd.date_range(start='05/10/2020', end ='01/03/2020',freq='D')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df3 = df.groupby(['date','Topic'])['cleaned_tweets'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df3.pivot( index='date',columns='Topic', values='cleaned_tweets').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted=pivoted.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted.to_csv(\"tweets_by_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(20,15)) \n",
    "\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "\n",
    "pivoted.plot(kind='line', x='date', y='Challenge', label='Challenge',ax=ax)\n",
    "pivoted.plot(kind='line', x='date', y='Coronavirus', label='Coronavirus',ax=ax)\n",
    "pivoted.plot(kind='line', x='date', y='Healthcare', label='Healthcare',ax=ax)\n",
    "pivoted.plot(kind='line', x='date', y='Learning', label='Learning',ax=ax)\n",
    "pivoted.plot(kind='line', x='date', y='Medical', label='Medical',ax=ax)\n",
    "pivoted.plot(kind='line', x='date', y='Rwanda', label='Rwanda',ax=ax)\n",
    "pivoted.plot(kind='line', x='date', y='Technology', label='Technology',ax=ax)\n",
    "pivoted.plot(kind='line', x='date', y='Webinar', label='Webinar',ax=ax)\n",
    "\n",
    "\n",
    "# ax.xaxis_date()\n",
    "# ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'));\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTER ANALYSIS WITH HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing a corpus for analysis and checking first 10 entries\n",
    "\n",
    "corpus=[]\n",
    "a=[]\n",
    "for i in range(len(df['cleaned_tweets'])):\n",
    "    a=df['cleaned_tweets'][i]\n",
    "    corpus.append(a)\n",
    "        \n",
    "print(corpus[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "import tempfile\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# init_notebook_mode(connected=True) #do not miss this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing common words and tokenizing\n",
    "list1 = ['RT','rt']\n",
    "stoplist = stopwords.words('english') + list(punctuation) + list1\n",
    "\n",
    "texts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# print(dictionary)\n",
    "# print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 2\n",
    "\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
    "corpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "\n",
    "#Show first n important word in the topics:\n",
    "lda.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_topics = 3\n",
    "\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
    "corpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "\n",
    "#Show first n important word in the topics:\n",
    "lda.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lda = {i: OrderedDict(lda.show_topic(i,10)) for i in range(total_topics)}\n",
    "data_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_lda = pd.DataFrame(data_lda)\n",
    "df_lda = df_lda.fillna(0).T\n",
    "print(df_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_lda=df_lda.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g=sns.clustermap(df_lda.corr(), center=0, standard_scale=1, cmap=\"coolwarm\", metric='cosine', linewidths=.75, figsize=(15, 15), col_cluster=False)\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "plt.show()\n",
    "#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# panel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\n",
    "# panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRELATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('tweets_by_date.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify continuous variables\n",
    "print(dataset.select_dtypes(['float']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Next, I want to compare the relationship (Correlation) between the identified topics in the dataset.\n",
    "\n",
    "sns.pairplot(dataset[['Challenge', 'Coronavirus', 'Healthcare', 'Learning', 'Medical',\n",
    "       'Rwanda', 'Technology', 'Webinar']])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is another visual representation of Correlation Matrix to use\n",
    "## SELECT COLORS https://python-graph-gallery.com/92-control-color-in-seaborn-heatmaps/\n",
    "\n",
    "\n",
    "corrMatrix = pivoted.corr()\n",
    "\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(corrMatrix, annot=False,\n",
    "            xticklabels=corrMatrix.columns,\n",
    "            yticklabels=corrMatrix.columns,\n",
    "            cmap='RdBu_r'\n",
    "            )\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is another visual representation of Correlation Matrix to use\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(corrMatrix, annot=True, linewidth = 0.5, cmap='Spectral_r',square=False, vmin = -1, vmax = 1)\n",
    "plt.title('Correlation matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTER ANALYSIS HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluter map\n",
    "sns.clustermap(corrMatrix, annot=True, linewidth = 0.5, cmap='RdBu_r',square=False, vmin = -1, vmax = 1)\n",
    "plt.title('Cluster Map')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
